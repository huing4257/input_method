# 输入法小作业实验报告

>  				      胡赢  2021012803  huing4257@163.com

## 实验目的

+ 通过语料库和汉字拼音对照表实现一个从一句话的拼音转换为汉字的程序
+ 比较不同实现方式和不同语料库的差别
+ 探索优化的方式

## 实验环境

### 环境：

+ python3.9

    + numpy

    + tqdm

### 文件结构与使用方式
+ 见README.md


## 语料库与处理方式

### 语料库

#### 仅使用新浪新闻语料库

+ 三元语法模型（未调节）
    + 句准确率：0.5298804780876494
    + 字准确率：0.8271251193887297

#### 加入百科类问答语料库和微博情绪分类技术评测

+ 三元语法模型（未调节）
    + 句准确率：0.5697211155378487
    + 字准确率：0.8809933142311366

#### 维基

+ 在最初时加入过中文wiki语料库，但使得准确率显著降低，后续没有继续采用

#### 分析

+ 由于测试用例均为与新闻、时事或是网络热点有关的句子，采用新闻和问答语料库对准确率有明显提高，而维基百科由于专有名词过多，词语组织方式也与日常用语不同，所以会导致准确率下降
+ 提高语料库规模能显著提高准确率。

### 处理方式

+ 使用正则表达式提取全部中文句子片段

+ 利用片段构造六个python字典
    + `dictionary`：存放拼音汉字对照表，格式为{拼音：汉字}

    + ` table_at_first`：存放句首汉字出现次数，格式为{char : count}

    + `table_1_to_1`:存放每个字的下一个字的出现次数，格式为{char : {next_char : count}}

    + `table_2_to_1`:存放每两个字的下一个字的出现次数，格式为 {two_char : {next_char : count}}

    + `table_one_count`:存放每个字的出现次数，格式为{char : count}

    + `table_two_count`:存放每两个字的出现次数，格式为{two_char : count}

+ 使用pickle库将以上字典序列化后保存在文件中，以便后续使用


## 语言模型

### 二元字法模型

+ 基于字的二元模型算法

#### 基本思路

#### 公式推导

+ 要估计句子出现概率，可使用以下公式：

$$
P(w_1w_2\dots w_n)= P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_1w_2)\cdots P(w_{n}|w_1\dots w_{n-1})
$$

+ 但是上述概率难以计算，故使用二元模型，公式如下：

$$
P(w_1w_2\dots w_n)= P(w_1)\cdot P(w_2|w_1)\cdots P(w_{n}|w_{n-1})
$$

+ 由于概率乘积可能会很小，故取对数防止下溢，最后所计算的表达式为
  $$
  \ln P(w_1)+\sum_{i=1}^{n-1} \ln P(w_{i+1}|w_1)
  $$

+ 通过搜索算法使其最大化即可。

#### 具体实现

+ 二元语法模型，即每个字的出现概率只与前一个字有关
+ 通过统计语料库中每个字在句首出现的频率，计算出句首第一个拼音各个字的概率
+ 通过统计语料库中每个字与前一个字共同出现的频率，计算出每个字与前一个字共同出现的概率
+ 使用 viterbi 算法动态规划计算最大概率路径：
    + 用一个二维数组`dp`记录到达每个字时的最大概率。
    + 用一个二维数组`pred`记录到达每个字时的最大概率路径的前一个字。
    + 句首第一个字的概率为句首第一个拼音各个字的概率。
    + 在每一步中，计算每个字与前一个字共同出现的概率与前一个字的最大概率的对数和，取最大值作为当前字的最大概率，并且记录当前字的最大概率路径的前一个字。
    + 计算结束后，取最后一个字的最大概率选项，再通过`pred`数组回溯得到最大概率路径，输出结果。

##### 准确率

+ 二元语法模型
    + 句准确率：0.40
    
    + 字准确率：0.84
    

### 三元字法模型

#### 基础模型

+ 所求表达式如下
  $$
  \ln P(w_1)+\ln P(w_2|w_1)+\sum_{i=3}^{n} \ln P(w_{i}|w_{i-2}w_{i-1})
  $$

+ 实现方式

    + 与二元类似，第二个字采用与二元相同的方式，后续的词计算概率时回溯两步。

+ 测试结果

    + 句准确率：0.5896414342629482
    + 字准确率：0.8758357211079274

#### 调节模型

+ 在步骤中计算概率时加入对字或词在整体语料库范围内频率的考察，对过于生僻的字或者词添加惩罚。
+ 所要最大化的表达式如下

$$
\ln P(w_1)+\ln P(w_2|w_1)+\sum_{i=3}^{n} (\ln P(w_{i}|w_{i-2}w_{i-1})+\frac{\ln P(w_{i})}{CHAR\_COEF}+\frac{ln P(w_
{i-2}w_{i-1})}{WORD\_COEF})
$$

##### 通过单个字符频率调节(CHAR_COEF)

+ CHAR_COEF = 1/2
    + 句准确率：0.4203187250996016
    + 字准确率：0.8477554918815664
+ CHAR_COEF = 2
    + 句准确率：0.5697211155378487
    + 字准确率：0.8809933142311366
+ CHAR_COEF = 4
    + 句准确率：0.5896414342629482
    + 字准确率：0.8857688634192932
+ CHAR_COEF = 8
    + 句准确率：0.5896414342629482
    + 字准确率：0.8815663801337154

##### 通过前一个词语频率调节（以字符频率系数为8为前提）

+ WORD_COEF = 2
    + 句准确率：0.5976095617529881
    + 字准确率：0.9025787965616046
+ WORD_COEF = 3
    + 句准确率：0.6075697211155379
    + 字准确率：0.9031518624641833
+ WORD_COEF = 4
    + 句准确率：0.603585657370518
    + 字准确率：0.899522445081184
+ WORD_COEF = 8
    + 句准确率：0.603585657370518
    + 字准确率：0.897612225405921

#### 最终模型

+ CHAR_COEF = 8， WORD_COEF = 3
    + 句准确率：0.6075697211155379
    
    + 字准确率：0.9031518624641833
    
      

## 具体案例分析

### 二元、三元、多元

+ 采用二元字法模型时，“中国和意大利都是历史悠久的文明古国”中的“意大利”始终无法正确输出，会输出类似“一大力”，因为如果仅分析每两个字的话，意大和大利出现频率均不会很高，只有能够分析三个字之间的联系后才能识别“意大利”这一词语。
+ 三元也会有类似的不足，如测试集中的“爱因斯坦”，斯坦由于出现频率较高能够正常输出，但“爱”“因”两个字如果不结合四个拼音一起分析的话，很难得到正确结果。“卡布奇诺”一词也是如此

### 实效性

+ 测试输入中许多词汇如“疫情”，“新冠”，“防控”有较大的实效性，由于较难找到2020年后的语料库、使用语料库时间均为18年以前，所以在判断这些词的时候几乎都会有错误。
+ 类似的词还有一些网络用语，如“爱豆”等。

### 特殊词语

+ 数字如“二十九”等，如果不加入特殊判断，很难正确输出。

  

## 实验总结

+ 本次实验中，我主要实现了基于二元、三元字法模型的中文输入法系统，通过对语料库的分析，我实现了对词语的概率计算
  ，以及对句子的最大概率路径的计算，最终实现了句子的输出。
+ 我此前很少使用python进行开发，所以在这个过程中我进一步熟悉了python的语法，以及一些常用的库的使用方法，如`numpy`,`argparser`,同时也学习了一些基本的数据结构使用，如`dict`、`list`等。
+ 通过这次作业，我对人工智能、搜索算法有了初步的认识，也对调整参数、优化算法有了一定的了解。



